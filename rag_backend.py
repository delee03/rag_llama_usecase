# from langchain_aws.chat_models import ChatBedrock
# from langchain_community.embeddings import BedrockEmbeddings
# from langchain_community.document_loaders import PyPDFLoader
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain.vectorstores import FAISS
# from langchain.indexes import VectorstoreIndexCreator
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.runnables import RunnablePassthrough

# # 1Ô∏è‚É£ T·∫°o index t·ª´ PDF
# def hr_index():
#     # 2. Load t√†i li·ªáu PDF
#     data_load = PyPDFLoader('https://www.upl-ltd.com/images/people/downloads/Leave-Policy-India.pdf')  

#     # 3. Chia nh·ªè vƒÉn b·∫£n th√†nh chunk
#     data_split = RecursiveCharacterTextSplitter(separators=["\n\n", "\n", " ", ""], chunk_size=100, chunk_overlap=10)

#     # 4. T·∫°o embeddings b·∫±ng Amazon Titan
#     data_embeddings = BedrockEmbeddings(
#         region_name='us-east-1',
#         credentials_profile_name='fuderrpham',
#         model_id='amazon.titan-embed-text-v2:0',
#     )
#     #amazon.titan-embed-text-v2:0

 
#     # 5. T·∫°o VectorStore (FAISS)
#     data_index = VectorstoreIndexCreator(
#         text_splitter=data_split,
#         embedding=data_embeddings,
#         vectorstore_cls=FAISS
#     )

#     # 6. T·∫°o index t·ª´ t√†i li·ªáu ƒë√£ load
#     db_index = data_index.from_loaders([data_load])
    
#     return db_index  # Tr·∫£ v·ªÅ index ch·ª©a vectorstore

# # 2Ô∏è‚É£ T·∫°o k·∫øt n·ªëi ƒë·∫øn Bedrock Claude-3.5
# def hr_llm():
#     return ChatBedrock(
#         credentials_profile_name='default',
#         model_id='anthropic.claude-3-sonnet-20240229-v1',
#         model_kwargs={
#             "max_tokens": 3000,
#             "temperature": 0.1,
#             "top_p": 0.9
#         }
#     )

# # 3Ô∏è‚É£ Truy v·∫•n Vector DB v√† g·ªçi LLM
# def hr_rag_response(index, question):
#     if not question.strip():
#         return "‚ö†Ô∏è Input kh√¥ng h·ª£p l·ªá: c√¢u h·ªèi tr·ªëng!"

#     print(f"üì© Input g·ª≠i l√™n Bedrock: {question}")  # Debug log

#     # Truy v·∫•n Vector DB ƒë·ªÉ t√¨m t√†i li·ªáu ph√π h·ª£p
#     retrieved_docs = index.vectorstore.similarity_search(question, k=3)  # Truy c·∫≠p v√†o vectorstore
#     retrieved_texts = "\n".join([doc.page_content for doc in retrieved_docs])

#     # T·∫°o prompt v·ªõi th√¥ng tin t·ª´ Vector DB
#     prompt = ChatPromptTemplate.from_messages([
#         ("system", "B·∫°n l√† tr·ª£ l√Ω AI h·ªó tr·ª£ HR. D∆∞·ªõi ƒë√¢y l√† th√¥ng tin t·ª´ Vector DB:\n{context}\n"),
#         ("human", "C√¢u h·ªèi: {question}\nTr·∫£ l·ªùi:")
#     ])

#     # K·∫øt h·ª£p d·ªØ li·ªáu ƒë·ªÉ g·ª≠i ƒë·∫øn model
#     chain = prompt | hr_llm() | RunnablePassthrough()

#     # G·ªçi model v·ªõi d·ªØ li·ªáu truy v·∫•n
#     response = chain.invoke({
#         "context": retrieved_texts,
#         "question": question
#     })

#     return response.content  # Tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi c·ªßa Claude



from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import BedrockEmbeddings
from langchain.vectorstores import FAISS
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chains import ConversationChain
from langchain_aws import ChatBedrock
from langchain_core.prompts import ChatPromptTemplate
import os

# üöÄ 1. Load & Index d·ªØ li·ªáu t·ª´ PDF
def create_pdf_index(pdf_path):
    """Load t√†i li·ªáu PDF, t·∫°o embeddings v√† l∆∞u v√†o FAISS"""
    # 1Ô∏è‚É£ Load t√†i li·ªáu PDF
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()

    # 2Ô∏è‚É£ Chia nh·ªè t√†i li·ªáu th√†nh c√°c ƒëo·∫°n vƒÉn
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    split_docs = text_splitter.split_documents(docs)

    # 3Ô∏è‚É£ T·∫°o embeddings s·ª≠ d·ª•ng Amazon Titan
    embeddings = BedrockEmbeddings(
        region_name="us-east-1",
        credentials_profile_name="default",
        model_id="amazon.titan-embed-text-v2:0"
    )

    # 4Ô∏è‚É£ T·∫°o FAISS VectorStore v√† l∆∞u index
    vectorstore = FAISS.from_documents(split_docs, embeddings)
    vectorstore.save_local("faiss_index")

    return vectorstore  # Tr·∫£ v·ªÅ FAISS index ƒë·ªÉ s·ª≠ d·ª•ng ngay

# üöÄ 2. Load FAISS index (n·∫øu ƒë√£ t·ªìn t·∫°i)
def load_pdf_index():
    """Load FAISS index n·∫øu ƒë√£ t·ªìn t·∫°i, n·∫øu kh√¥ng s·∫Ω tr·∫£ v·ªÅ None"""
    embeddings = BedrockEmbeddings(
        region_name="us-east-1",
        credentials_profile_name="default",
        model_id="amazon.titan-embed-text-v2:0"
    )

    if os.path.exists("faiss_index"):
        return FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
    else:
        return None  # N·∫øu ch∆∞a c√≥ index, c·∫ßn ch·∫°y create_pdf_index()

# üöÄ 3. K·∫øt n·ªëi AWS Bedrock LLaMA 3
def demo_chatbot():
    """T·∫°o k·∫øt n·ªëi t·ªõi LLaMA 3 tr√™n AWS Bedrock"""
    return ChatBedrock(
        credentials_profile_name='default',
        model_id='meta.llama3-70b-instruct-v1:0',
        model_kwargs={
            "max_gen_len": 2048,
            "temperature": 0.1,
            "top_p": 0.9
        }
    )

# üöÄ 4. T·∫°o b·ªô nh·ªõ h·ªôi tho·∫°i v·ªõi LLM
def demo_memory():
    """T·∫°o Conversation Memory v·ªõi Bedrock"""
    llm_data = demo_chatbot()  # G·ªçi LLaMA 3 t·ª´ AWS Bedrock
    memory = ConversationSummaryBufferMemory(llm=llm_data, max_token_limit=300)
    return memory

# üöÄ 5. K·∫øt h·ª£p FAISS + LLaMA 3 ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi (RAG)
def rag_conversation(user_input, memory):
    """Truy v·∫•n FAISS, k·∫øt h·ª£p v·ªõi LLaMA 3 ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi"""
    faiss_index = load_pdf_index()
    
    if faiss_index is None:
        return "‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y index! Vui l√≤ng t·∫£i PDF tr∆∞·ªõc."

    # üîç 1. T√¨m t√†i li·ªáu ph√π h·ª£p v·ªõi c√¢u h·ªèi
    retrieved_docs = faiss_index.similarity_search(user_input, k=3)
    retrieved_texts = "\n".join([doc.page_content for doc in retrieved_docs])

    # üìå 2. T·∫°o prompt v·ªõi th√¥ng tin t√¨m ƒë∆∞·ª£c
    prompt = ChatPromptTemplate.from_messages([
        ("system", "B·∫°n l√† tr·ª£ l√Ω AI chuy√™n nghi·ªáp. D∆∞·ªõi ƒë√¢y l√† n·ªôi dung t·ª´ t√†i li·ªáu PDF:\n{context}\n"),
        ("human", "C√¢u h·ªèi: {question}\nTr·∫£ l·ªùi:")
    ])

    # üîÑ 3. K·∫øt h·ª£p d·ªØ li·ªáu v·ªõi model
    chain = prompt | demo_chatbot()

    # üî• 4. G·ªçi model LLaMA 3 v·ªõi d·ªØ li·ªáu
    response = chain.invoke({
        "context": retrieved_texts,
        "question": user_input
    })

    # üìù 5. C·∫≠p nh·∫≠t b·ªô nh·ªõ h·ªôi tho·∫°i
    memory.save_context({"input": user_input}, {"output": response.content})

    return response.content  # Tr·∫£ v·ªÅ ph·∫£n h·ªìi c·ªßa AI
