
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import BedrockEmbeddings
from langchain.vectorstores import FAISS
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chains import ConversationChain
from langchain_aws import ChatBedrock
from langchain_core.prompts import ChatPromptTemplate
import os

# ğŸš€ 1. Load & Index dá»¯ liá»‡u tá»« PDF
def create_pdf_index(pdf_path):
    """Load tÃ i liá»‡u PDF, táº¡o embeddings vÃ  lÆ°u vÃ o FAISS"""
    # 1ï¸âƒ£ Load tÃ i liá»‡u PDF
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()

    # 2ï¸âƒ£ Chia nhá» tÃ i liá»‡u thÃ nh cÃ¡c Ä‘oáº¡n vÄƒn
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    split_docs = text_splitter.split_documents(docs)

    # 3ï¸âƒ£ Táº¡o embeddings sá»­ dá»¥ng Amazon Titan
    embeddings = BedrockEmbeddings(
        region_name="us-east-1",
        credentials_profile_name="default",
        model_id="amazon.titan-embed-text-v2:0"
    )

    # 4ï¸âƒ£ Táº¡o FAISS VectorStore vÃ  lÆ°u index
    vectorstore = FAISS.from_documents(split_docs, embeddings)
    vectorstore.save_local("faiss_index")

    return vectorstore  # Tráº£ vá» FAISS index Ä‘á»ƒ sá»­ dá»¥ng ngay

# ğŸš€ 2. Load FAISS index (náº¿u Ä‘Ã£ tá»“n táº¡i)
def load_pdf_index():
    """Load FAISS index náº¿u Ä‘Ã£ tá»“n táº¡i, náº¿u khÃ´ng sáº½ tráº£ vá» None"""
    embeddings = BedrockEmbeddings(
        region_name="us-east-1",
        credentials_profile_name="default",
        model_id="amazon.titan-embed-text-v2:0"
    )

    if os.path.exists("faiss_index"):
        return FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
    else:
        return None  # Náº¿u chÆ°a cÃ³ index, cáº§n cháº¡y create_pdf_index()

# ğŸš€ 3. Káº¿t ná»‘i AWS Bedrock LLaMA 3
def demo_chatbot():
    """Táº¡o káº¿t ná»‘i tá»›i LLaMA 3 trÃªn AWS Bedrock"""
    return ChatBedrock(
        credentials_profile_name='default',
        model_id='meta.llama3-70b-instruct-v1:0',
        model_kwargs={
            "max_gen_len": 2048,
            "temperature": 0.3,
            "top_p": 0.9
        }
    )

# ğŸš€ 4. Táº¡o bá»™ nhá»› há»™i thoáº¡i vá»›i LLM
def demo_memory():
    """Táº¡o Conversation Memory vá»›i Bedrock"""
    llm_data = demo_chatbot()  # Gá»i LLaMA 3 tá»« AWS Bedrock
    memory = ConversationSummaryBufferMemory(llm=llm_data, max_token_limit=300)
    return memory

# ğŸš€ 5. Káº¿t há»£p FAISS + LLaMA 3 Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i (RAG)
def rag_conversation(user_input, memory):
    """Truy váº¥n FAISS, káº¿t há»£p vá»›i LLaMA 3 Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i"""
    faiss_index = load_pdf_index()
    
    if faiss_index is None:
        return "âš ï¸ KhÃ´ng tÃ¬m tháº¥y index! Vui lÃ²ng táº£i PDF trÆ°á»›c."

    # ğŸ” 1. TÃ¬m tÃ i liá»‡u phÃ¹ há»£p vá»›i cÃ¢u há»i
    retrieved_docs = faiss_index.similarity_search(user_input, k=3)
    retrieved_texts = "\n".join([doc.page_content for doc in retrieved_docs])

    # ğŸ“Œ 2. Táº¡o prompt vá»›i thÃ´ng tin tÃ¬m Ä‘Æ°á»£c
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn nghiá»‡p. DÆ°á»›i Ä‘Ã¢y lÃ  ná»™i dung tá»« tÃ i liá»‡u PDF:\n{context}\n"),
        ("human", "CÃ¢u há»i: {question}\nTráº£ lá»i:")
    ])

    # ğŸ”„ 3. Káº¿t há»£p dá»¯ liá»‡u vá»›i model
    chain = prompt | demo_chatbot()

    # ğŸ”¥ 4. Gá»i model LLaMA 3 vá»›i dá»¯ liá»‡u
    response = chain.invoke({
        "context": retrieved_texts,
        "question": user_input
    })

    # ğŸ“ 5. Cáº­p nháº­t bá»™ nhá»› há»™i thoáº¡i
    memory.save_context({"input": user_input}, {"output": response.content})

    return response.content  # Tráº£ vá» pháº£n há»“i cá»§a AI
